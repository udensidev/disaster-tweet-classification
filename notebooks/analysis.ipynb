{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nltk.download('stopwords')` function call ensures that the NLTK stopwords dataset is downloaded and available for use. Stopwords are common words (such as \"and\", \"the\", \"is\") that are often removed during text preprocessing to focus on more meaningful words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions Defined\n",
    "\n",
    "### `load_data`\n",
    "\n",
    "The `load_data` function is responsible for loading the training and test datasets from CSV files. It reads the data into pandas DataFrames and returns them for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load training and test data from CSV files\n",
    "    Returns:\n",
    "        train_df: pandas DataFrame containing training data\n",
    "        test_df: pandas DataFrame containing test data\n",
    "    \"\"\"\n",
    "    train_df = pd.read_csv('/Users/mmesoma/personal-projects/kaggle-competition-nlp-disaster-tweets/data/train.csv')\n",
    "    test_df = pd.read_csv('/Users/mmesoma/personal-projects/kaggle-competition-nlp-disaster-tweets/data/test.csv')\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `clean_text`\n",
    "\n",
    "The `clean_text` function preprocesses text data by performing several cleaning steps. It removes URLs, mentions, and non-alphanumeric characters (except spaces and numbers), converts the text to lowercase, tokenizes it, removes stopwords, and applies stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean text data by removing URLs, mentions, hashtags, and non-alphanumeric characters\n",
    "    Args:\n",
    "        text: string\n",
    "    \"\"\"\n",
    "    \n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    text = text.lower().strip()\n",
    "    words = text.split()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `vectorize_text`\n",
    "\n",
    "The `vectorize_text` function applies TF-IDF vectorization to the training, validation, and test text datasets. It converts the text data into numerical feature vectors, which are suitable for machine learning models. The function takes in the training, validation, and test text data, along with optional parameters for max_features and ngram_range, and returns the vectorized feature matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(train_text, val_text, test_text, max_features=5000, ngram_range=(1, 2)):\n",
    "    \"\"\"\n",
    "    Vectorize text data using TF-IDF\n",
    "    Args:\n",
    "        train_text: pandas Series containing training text data\n",
    "        val_text: pandas Series containing validation text data\n",
    "        test_text: pandas Series containing test text data\n",
    "        max_features: int, default=5000\n",
    "        ngram_range: tuple, default=(1, 2)\n",
    "    Returns:\n",
    "        X_train_vec: sparse matrix for training data\n",
    "        X_val_vec: sparse matrix for validation data\n",
    "        X_test_vec: sparse matrix for test data\n",
    "    \"\"\"\n",
    "    tfidf = TfidfVectorizer(max_features=max_features, ngram_range=ngram_range)\n",
    "    X_train_vec = tfidf.fit_transform(train_text)\n",
    "    X_val_vec = tfidf.transform(val_text)\n",
    "    X_test_vec = tfidf.transform(test_text)\n",
    "    return X_train_vec, X_val_vec, X_test_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3263 entries, 0 to 3262\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        3263 non-null   int64 \n",
      " 1   keyword   3237 non-null   object\n",
      " 2   location  2158 non-null   object\n",
      " 3   text      3263 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 102.1+ KB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "We apply the `clean_text` function to preprocess the text data in both the training and test datasets. This step ensures that the text data is cleaned and standardized before further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning text data...\n",
      "\n",
      "Text data cleaned successfully.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCleaning text data...\")\n",
    "train_df['cleaned_text'] = train_df['text'].apply(clean_text)\n",
    "test_df['cleaned_text'] = test_df['text'].apply(clean_text)\n",
    "print(\"\\nText data cleaned successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquak may allah forgiv us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>resid ask shelter place notifi offic evacu she...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>peopl receiv wildfir evacu order california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got sent photo rubi alaska smoke wildfir pour ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                       cleaned_text  \n",
       "0       1          deed reason earthquak may allah forgiv us  \n",
       "1       1               forest fire near la rong sask canada  \n",
       "2       1  resid ask shelter place notifi offic evacu she...  \n",
       "3       1        peopl receiv wildfir evacu order california  \n",
       "4       1  got sent photo rubi alaska smoke wildfir pour ...  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we split the training dataset into training and validation sets using an 80-20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_df[\"cleaned_text\"], \n",
    "                                                  train_df[\"target\"], \n",
    "                                                  test_size=0.2, \n",
    "                                                  random_state=79)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We vectorized the cleaned text data using the `vectorize_text` function, which applies TF-IDF vectorization to the training, validation, and test datasets. This step converts the text data into numerical feature vectors suitable for machine learning models. The following code snippet demonstrates the vectorization process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vec, X_val_vec, X_test_vec = vectorize_text(X_train, X_val, test_df['cleaned_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We utilized the Synthetic Minority Over-sampling Technique (SMOTE) to address class imbalance in our training dataset. By generating synthetic samples for the minority class, SMOTE helps to balance the class distribution, which can improve model performance and generalization. The following code snippet demonstrates the application of SMOTE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "smote = SMOTE(random_state=79)\n",
    "X_train_bal, y_train_bal = smote.fit_resample(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll train a Bernoulli Naive Bayes model using the balanced training data and evaluate its performance on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8168089297439265\n"
     ]
    }
   ],
   "source": [
    "BERNmodel = BernoulliNB().fit(X_train_bal, y_train_bal)\n",
    "pred_val = BERNmodel.predict(X_val_vec)\n",
    "accuracy = accuracy_score(y_val, pred_val)\n",
    "print(f\"Validation accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we make predictions with our trained model. The predicted values are saved in `submission.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "File created: submission.csv\n"
     ]
    }
   ],
   "source": [
    "pred_test = BERNmodel.predict(X_test_vec)\n",
    "submission = pd.DataFrame({'id': test_df['id'], 'target': pred_test})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"\\nFile created: submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
